# π® Jupyter Notebookμ—μ„ GPU μ‚¬μ©ν•λ” λ°©λ²•

## β οΈ μ¤‘μ”: ν„μ¬ μƒνƒ

### ν„μ¬ ν”„λ΅μ νΈλ“¤μ€ GPUλ¥Ό μ‚¬μ©ν•μ§€ μ•μµλ‹λ‹¤!

**μ΄μ :**
- β… **OpenAI API** μ‚¬μ© μ¤‘ β†’ ν΄λΌμ°λ“μ—μ„ μ‹¤ν–‰ (λ΅μ»¬ GPU λ¶ν•„μ”)
- β… **Anthropic API** μ‚¬μ© μ¤‘ β†’ ν΄λΌμ°λ“μ—μ„ μ‹¤ν–‰ (λ΅μ»¬ GPU λ¶ν•„μ”)
- β… **Google AI API** μ‚¬μ© μ¤‘ β†’ ν΄λΌμ°λ“μ—μ„ μ‹¤ν–‰ (λ΅μ»¬ GPU λ¶ν•„μ”)

**κ²°λ΅ :**
- ν„μ¬ λ…ΈνΈλ¶λ“¤μ„ μ‹¤ν–‰ν•΄λ„ **CPUλ§ μ‚¬μ©**λ©λ‹λ‹¤
- GPUλ” **μ„¤μΉλ§ λμ–΄ μκ³  μ‚¬μ©λμ§€ μ•μµλ‹λ‹¤**
- API νΈμ¶μ€ μΈν„°λ„·μ„ ν†µν•΄ ν΄λΌμ°λ“ μ„λ²„μ—μ„ μ²λ¦¬λ©λ‹λ‹¤

---

## π€ GPUλ¥Ό μ‚¬μ©ν•λ ¤λ©΄?

### λ°©λ²• 1: Jupyter μ»¤λ„ λ³€κ²½ (μ¤€λΉ„ μ™„λ£!)

#### 1λ‹¨κ³„: Jupyter Notebook μ‹¤ν–‰
```bash
conda activate pytorch_gpu_312
jupyter notebook
```

#### 2λ‹¨κ³„: λ…ΈνΈλ¶μ—μ„ μ»¤λ„ λ³€κ²½
1. Jupyter Notebook μ—΄κΈ°
2. μƒλ‹¨ λ©”λ‰΄: **Kernel** β†’ **Change kernel**
3. **"Python 3.12 (PyTorch GPU)"** μ„ νƒ

#### 3λ‹¨κ³„: GPU μ‚¬μ© μ½”λ“ μ¶”κ°€
```python
import torch

# GPU ν™•μΈ
print(f"CUDA μ‚¬μ© κ°€λ¥: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")

# λ””λ°”μ΄μ¤ μ„¤μ •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"μ‚¬μ© μ¤‘μΈ λ””λ°”μ΄μ¤: {device}")

# GPUμ—μ„ ν…μ„ μ—°μ‚°
x = torch.randn(1000, 1000, device=device)
y = torch.randn(1000, 1000, device=device)
z = torch.matmul(x, y)
print(f"κ²°κ³Ό ν…μ„ λ””λ°”μ΄μ¤: {z.device}")
```

---

### λ°©λ²• 2: gpu_utils.py μ‚¬μ© (λ” κ°„λ‹¨!)

```python
# λ…ΈνΈλ¶ μ²« μ…€μ— μ¶”κ°€
import sys
sys.path.append('..')  # ν”„λ΅μ νΈ λ£¨νΈλ΅ κ²½λ΅ μ¶”κ°€

from gpu_utils import get_device, print_gpu_info

# GPU μ •λ³΄ μ¶λ ¥
print_gpu_info()

# λ””λ°”μ΄μ¤ μλ™ μ„ νƒ (GPU μμΌλ©΄ GPU, μ—†μΌλ©΄ CPU)
device = get_device(prefer_gpu=True)

# μ΄μ  PyTorch μ½”λ“μ—μ„ device μ‚¬μ©
import torch
x = torch.randn(100, 100, device=device)
```

---

## π“ GPUκ°€ μ‹¤μ λ΅ ν•„μ”ν• κ²½μ°

### 1. λ΅μ»¬ LLM μ‹¤ν–‰
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

device = torch.device('cuda')

# λ¨λΈ λ΅λ“ (GPUλ΅)
model = AutoModelForCausalLM.from_pretrained(
    "beomi/Llama-3-Open-Ko-8B",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("beomi/Llama-3-Open-Ko-8B")

# μ¶”λ΅ 
inputs = tokenizer("μ•λ…•ν•μ„Έμ”", return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### 2. λ΅μ»¬ μ„λ² λ”© λ¨λΈ
```python
from sentence_transformers import SentenceTransformer
import torch

device = torch.device('cuda')

# μ„λ² λ”© λ¨λΈ λ΅λ“ (GPUλ΅)
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
model = model.to(device)

# μ„λ² λ”© μƒμ„±
sentences = ["μ•λ…•ν•μ„Έμ”", "Hello", "γ“γ‚“γ«γ΅γ―"]
embeddings = model.encode(sentences, device=device)
print(embeddings.shape)
```

### 3. νμΈνλ‹/ν•™μµ
```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

device = torch.device('cuda')

# λ¨λΈμ„ GPUλ΅
model = YourModel().to(device)
optimizer = torch.optim.Adam(model.parameters())

# ν•™μµ λ£¨ν”„
for epoch in range(num_epochs):
    for batch in dataloader:
        inputs, labels = batch
        inputs = inputs.to(device)  # λ°μ΄ν„°λ¥Ό GPUλ΅
        labels = labels.to(device)
        
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

## π” GPU μ‚¬μ© ν™•μΈ λ°©λ²•

### λ°©λ²• 1: μ½”λ“λ΅ ν™•μΈ
```python
import torch

print("=" * 50)
print("GPU μ‚¬μ© ν™•μΈ")
print("=" * 50)
print(f"PyTorch λ²„μ „: {torch.__version__}")
print(f"CUDA μ‚¬μ© κ°€λ¥: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"CUDA λ²„μ „: {torch.version.cuda}")
    print(f"GPU μ΄λ¦„: {torch.cuda.get_device_name(0)}")
    print(f"GPU λ©”λ¨λ¦¬: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
    
    # ν„μ¬ GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰
    print(f"ν• λ‹Ήλ λ©”λ¨λ¦¬: {torch.cuda.memory_allocated(0) / (1024**2):.2f} MB")
    print(f"μΊμ‹λ λ©”λ¨λ¦¬: {torch.cuda.memory_reserved(0) / (1024**2):.2f} MB")
else:
    print("GPUλ¥Ό μ‚¬μ©ν•  μ μ—†μµλ‹λ‹¤.")
print("=" * 50)
```

### λ°©λ²• 2: NVIDIA λ„κµ¬ μ‚¬μ©
```bash
# GPU μ‚¬μ©λ¥  μ‹¤μ‹κ°„ λ¨λ‹ν„°λ§
nvidia-smi -l 1

# λλ” ν• λ²λ§ ν™•μΈ
nvidia-smi
```

---

## π“ ν„μ¬ ν”„λ΅μ νΈλ³„ μƒνƒ

### 1. `assistant-question-answering/`
- **API μ‚¬μ©**: OpenAI API
- **GPU ν•„μ”**: β μ—†μ
- **ν„μ¬ μƒνƒ**: CPUλ§ μ‚¬μ© (μ •μƒ)

### 2. `baemin-recommendation/`
- **API μ‚¬μ©**: OpenAI API
- **GPU ν•„μ”**: β μ—†μ
- **ν„μ¬ μƒνƒ**: CPUλ§ μ‚¬μ© (μ •μƒ)

### 3. `kakaotalk-summarization/`
- **API μ‚¬μ©**: OpenAI, Anthropic, Google AI API
- **GPU ν•„μ”**: β μ—†μ
- **ν„μ¬ μƒνƒ**: CPUλ§ μ‚¬μ© (μ •μƒ)

### 4. `prompt-engineering/`
- **API μ‚¬μ©**: OpenAI API
- **GPU ν•„μ”**: β μ—†μ
- **ν„μ¬ μƒνƒ**: CPUλ§ μ‚¬μ© (μ •μƒ)

### 5. `retrieval-augmented-generation/`
- **API μ‚¬μ©**: OpenAI API (Embeddings)
- **GPU ν•„μ”**: β μ—†μ
- **ν„μ¬ μƒνƒ**: CPUλ§ μ‚¬μ© (μ •μƒ)
- **μ°Έκ³ **: λ΅μ»¬ μ„λ² λ”© λ¨λΈ μ‚¬μ© μ‹ GPU ν•„μ”

### 6. `yanolja-summarization/`
- **API μ‚¬μ©**: OpenAI API
- **GPU ν•„μ”**: β μ—†μ
- **ν„μ¬ μƒνƒ**: CPUλ§ μ‚¬μ© (μ •μƒ)

---

## π’΅ μ”μ•½

### ν„μ¬ μƒν™©:
- β… GPU ν™κ²½ μ„¤μ • μ™„λ£ (`pytorch_gpu_312`)
- β… PyTorch + CUDA μ„¤μΉ μ™„λ£
- β… Jupyter μ»¤λ„ λ“±λ΅ μ™„λ£
- β **ν•μ§€λ§ ν„μ¬ ν”„λ΅μ νΈλ“¤μ€ GPUλ¥Ό μ‚¬μ©ν•μ§€ μ•μ**

### μ΄μ :
- λ¨λ“  ν”„λ΅μ νΈκ°€ **ν΄λΌμ°λ“ API** μ‚¬μ©
- APIλ” **μ›κ²© μ„λ²„**μ—μ„ μ‹¤ν–‰
- λ΅μ»¬ GPUλ” **ν•„μ” μ—†μ**

### GPUλ¥Ό μ‚¬μ©ν•λ ¤λ©΄:
1. **λ΅μ»¬ λ¨λΈ μ‚¬μ©** (Llama, Mistral λ“±)
2. **λ΅μ»¬ μ„λ² λ”© λ¨λΈ μ‚¬μ©** (sentence-transformers λ“±)
3. **λ¨λΈ νμΈνλ‹/ν•™μµ**
4. **λ€λ‰ ν…μ„ μ—°μ‚°**

### μ»¤λ„ λ³€κ²½ λ°©λ²•:
1. Jupyter Notebook μ‹¤ν–‰: `conda activate pytorch_gpu_312 && jupyter notebook`
2. μ»¤λ„ λ³€κ²½: **Kernel** β†’ **Change kernel** β†’ **"Python 3.12 (PyTorch GPU)"**
3. GPU μ½”λ“ μ‘μ„± (μ„ μμ  μ°Έμ΅°)

---

**μ‘μ„±μΌ**: 2025-10-06  
**GPU**: NVIDIA GeForce RTX 3060 Laptop GPU (6GB)  
**ν™κ²½**: `pytorch_gpu_312` (Python 3.12.11, PyTorch 2.5.1+cu121)

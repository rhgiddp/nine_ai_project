{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG 데이터 확보\n",
    "- Allganize의 RAG 데이터 중 금융 도메인 데이터 200개 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/allganize/rag-ko\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id='allganize/rag-ko',\n",
    "    repo_type='dataset',\n",
    "    local_dir='./res/rag-ko',\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 설명\n",
    "- 질문과 정답이 있고, 답변에 필요한 정보(Context)가 질문 별로 3개씩 존재\n",
    "  - 3개의 정보 중 하나가 실제로 필요한 정보이고, 나머지 2개는 필요하지 않은 정보\n",
    "- Column 별 설명\n",
    "  - system: 시스템 프롬프트 (Context 3개가 여기에 있음)\n",
    "  - humman: 질문\n",
    "  - answer_context_summary: 실제로 필요한 Context\n",
    "  - answer_position: 3개의 Context 중 몇 번째가 실제로 필요한 Context인지\n",
    "  - answer: 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_parquet('./res/rag-ko/data/test-00000-of-00001.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0]['system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0]['answer_context_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시스템 프롬프트에서 3개의 Context를 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_contexts(system_prompt):\n",
    "    context_match = re.search(r'CONTEXT=\"\"\"\\n(.*?)\"\"\"', system_prompt, re.DOTALL)\n",
    "    if not context_match:\n",
    "        return []\n",
    "\n",
    "    context_text = context_match.group(1)\n",
    "\n",
    "    contexts = re.findall(r'\\(context \\d+\\)=(.*?)(?=\\n\\(context \\d+\\)=|\\Z)', context_text, re.DOTALL)\n",
    "\n",
    "    cleaned_contexts = []\n",
    "    for context in contexts:\n",
    "        lines = context.split('\\n')\n",
    "        cleaned_lines = [line for line in lines if not line.strip().startswith('Title:')]\n",
    "        cleaned_context = '\\n'.join(cleaned_lines).strip()\n",
    "        cleaned_contexts.append(cleaned_context)\n",
    "\n",
    "    return cleaned_contexts\n",
    "\n",
    "system_prompt = df.iloc[0]['system']\n",
    "\n",
    "extracted_contexts = extract_contexts(system_prompt)\n",
    "\n",
    "for i, context in enumerate(extracted_contexts, 1):\n",
    "    print(f\"Context {i}:\")\n",
    "    print(context)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_contexts[2] == df.iloc[0]['answer_context_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "for i in range(len(df)):\n",
    "    system_prompt = df.iloc[i]['system']\n",
    "    extracted_contexts = extract_contexts(system_prompt)\n",
    "    contexts.append(extracted_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [df.iloc[i]['human'] for i in range(len(df))]\n",
    "contexts_answers_idxs = [df.iloc[i]['answer_position'] - 1 for i in range(len(df))]\n",
    "contexts_answers = [df.iloc[i]['answer_context_summary'] for i in range(len(df))]\n",
    "answers = [df.iloc[i]['answer'] for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "rag_data = {\n",
    "    'questions': questions,\n",
    "    'contexts': contexts,\n",
    "    'contexts_answer_idx': contexts_answers_idxs,\n",
    "    'contexts_answers': contexts_answers,\n",
    "    'answers': answers\n",
    "}\n",
    "\n",
    "with open('./res/rag_data.pkl', 'wb') as f:\n",
    "    pickle.dump(rag_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "- 정답이 있는 상황이라 가장 정확한 평가\n",
    "- Reference 또는 정답이 있을 때만 사용 가능\n",
    "  - 실제로는 없는 경우가 대부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_embedding, cosine_similarity\n",
    "\n",
    "\n",
    "embed_q = get_embedding(questions[0], model='text-embedding-3-large')\n",
    "embed_c0 = get_embedding(contexts[0][0], model='text-embedding-3-large')\n",
    "embed_c1 = get_embedding(contexts[0][1], model='text-embedding-3-large')\n",
    "embed_c2 = get_embedding(contexts[0][2], model='text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(embed_q, embed_c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(embed_q, embed_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(embed_q, embed_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS\n",
    "- 정답이 없는 경우에도 사용이 가능한 평가 라이브러리\n",
    "- 기본으로 제공하는 평가용 System Prompt는 영문 기반이라 엄청 정확하진 않은 편"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "import os\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, context_relevancy, context_recall, context_precision\n",
    "\n",
    "\n",
    "data_samples = {\n",
    "    'question': [questions[0]],\n",
    "    'answer': ['hi'],\n",
    "    'contexts' : [contexts[1]],\n",
    "    'ground_truth': [df.iloc[0]['answer']]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(dataset,metrics=[context_recall, context_precision])\n",
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[i]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    data_samples = {\n",
    "        'question': [questions[i]],\n",
    "        'answer': ['hi'],\n",
    "        'contexts' : [contexts[i]],\n",
    "        'ground_truth': [df.iloc[i]['answer']]\n",
    "    }\n",
    "\n",
    "    dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "    score = evaluate(dataset,metrics=[context_recall, context_precision])\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorDB\n",
    "- 임베딩을 저장하고 빠르게 호출 할 수 있는 데이터베이스\n",
    "  - 입출력값 측면에서는 ChatGPT Embedding API 호출와 동일\n",
    "  - 내부적으로는 kNN을 쓸 수도 있고 aNN을 쓸 수도 있음\n",
    "- kNN vs. aNN?\n",
    "  - 이번 챕터에서 진행한 방식 = k-Nearest Neighbor (k=1)\n",
    "    - Prompt와 유사한 Neighbor(이웃)을 k개 찾는 것\n",
    "    - k가 일정 수치 넘어갈 수록 속도가 느려지는 단점이 있음\n",
    "  - aNN은 정확도를 희생하고 속도를 챙긴 방법 (a = Approximate = 근사)\n",
    "    - 대규모 데이터 안에서 검색해야 하는 경우 많이 사용되는 방법론\n",
    "- 그 외에는 임베딩 제공의 안정성 등을 강화\n",
    "  - ex. 모니터링 기능\n",
    "\n",
    "![Vector Search](./res/vector_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG에 대한 현실 및 제약\n",
    "- RAG는 추가적인 LLM 학습이 필요 없는 가성비가 좋은 방법론\n",
    "  - 난이도가 높을 경우 RAG에 사용하는 임베딩을 학습해야 할 수는 있음\n",
    "- 다만, RAG를 활용함으로서 LLM에서 LLM + RAG 2-step으로 바뀜\n",
    "  - 따라서 RAG에서 잘못된 정보 가져오면 Risk 발생\n",
    "  - 보통 더 작은 (임베딩) 모델 활용하기 때문에 RAG 자체에도 Risk가 존재\n",
    "    - ex. RAG에서 잘못된 정보를 가져와 할루시네이션 발생\n",
    "- 최근 들어서 LLM의 Input Context 길이가 1M 이상으로 늘기 시작\n",
    "  - 별도의 RAG 로직을 사용하지 않고, 모든 정보를 LLM 입력에 넣는 방식도 옵션이 될 가능성도 존재\n",
    "  - 아래는 The Needle in a Haystack\n",
    "    - 입력을 주고 입력 내에서 특정 텍스트를 찾는 테스트\n",
    "\n",
    "![Vector Search](./res/needle_haystack.webp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

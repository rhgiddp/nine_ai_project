{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/XZkxtOqiNCvg1Oz6Uft2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Prompt Engineering 파트 실습 2 - Prompt Development Cycle\n","\n","본격적으로 Prompt Engineering 실습 들어가기 전에 그것보다도 훨씬 더 중요한 프레임워크 소개\n","\n","- 일단 Prompt Engineering 기법들 이것저것 했을 때, 많은 시간을 투입해도 별다른 결과가 없을 수 있음\n","- 특히 이게 더 좋은 Prompt인가 판단하기도 어려울 수 있음\n","- Prompt Engineering은 AI 모델 학습과 다르게 성능 변화가 좀 더 Discrete한 영역 일 수 있음\n","\n","태스크에 맞춰서 명확한 평가 기준치와 기준치를 확인 할 수 있는 샘플 데이터가 필요!"],"metadata":{"id":"CQ-mC-oZmMeU"}},{"cell_type":"markdown","source":["## Prompt Development Cycle\n","\n","Prompt Engineering 프로세스를 테스트 기반 방법론으로 체계화하여 최상의 성능 달성하는 방법론\n","\n","- 테스트 케이스와 평가 기준 - 매우 중요!\n","- 평가 기준 설정\n","- 태스크마다 다름\n","  - 정답이 있는 경우(Supervised)와 없는 경우\n","  - ROUGE\n","  - LLM-as-a-judge\n","  - Blind Testing\n","  - ELO Rating\n","- Baseline Prompt 생성\n","- 테스트 및 고도화\n","\n","= AI 모델 학습을 통한 고도화와 비슷함\n","\n","![](https://drive.google.com/uc?export=view&id=1j4jxS4Yp_TnDSu42WTcjUv1dap2WuEm6)\n","\n","출처: Anthropic"],"metadata":{"id":"S6rR86GKpfSp"}},{"cell_type":"markdown","source":["Prompt Development Cycle은 아래 6단계로 나눠서 볼 수 있음\n","\n","1. 명확한 평가 기준 설정\n","2. 평가를 진행 할 테스트 케이스 선정 (엣지 케이스 포함)\n","3. Baseline Prompt 선정\n","  - 고도화 과정 후에 비교를 위한 대조군 설정\n","4. (반복) 테스트 케이스에 대해 평가 진행\n","5. (반복) Prompt 수정\n","6. Prompt 완성\n","  - Baseline 대비 어떤 지표에서 얼마나 개선되었는지"],"metadata":{"id":"O_ImXKhv8wNI"}},{"cell_type":"markdown","source":["### 1. 명확한 평가 기준 설정\n","\n","1-1. 태스크 정의\n","- 요약, Q&A, 코드 생성, 글쓰기 등등\n","- 각 태스크 별로 사용되는 평가 기준 및 지표들이 다를 수 있음\n","  - 동일한 요약 안에서도 대화 요약, 문서 요약 등이 존재\n","  - Q&A도 In-Domain, Out-of-Domain으로 나뉠 수도 있음\n","- 어떤 문제를 풀어야하는지 태스크를 구체적으로 명확하게 정의하는 것이 첫 단계!\n","\n","1-2. 평가 기준 설정\n","1. 성능\n","  - 태스크에서 정확히 어느 정도의 품질이 필요한 지를 정의하는 객관적인 기준\n","  - 태스크마다 평가 기준 다를 수 있고 여러 개의 평가 지표들이 있을 수 있음\n","    - 예시1. 객관식 질문에 대답하는 태스크\n","      - 정확도(Accuracy)\n","    - 예시2. 요약 태스크\n","      - 정답과 모델 출력값 간의 문자열 비교 (Exact Match or Partial Match)\n","2. 응답 속도 (Latency)\n","  - LLM에서 Latency 정의: Prompt 입력 후 응답 완료까지 걸리는 시간 (주요 용어)\n","  - 응답 시간. 실시간 및 비실시간 여부에 따라 기준치가 높아지거나 낮아질 수 있음\n","3. 비용\n","  - 모델 가격, 사용되는 평균 입력 및 출력 토큰 수, 호출 수 등을 고려한 예상 비용\n","\n","우선순위는 서비스마다 다를 수 있는데, 아무래도 품질이 불충분하면 서비스 가능성이 없기 때문에 위 순서대로 우선순위를 보시는 것도 좋은 시작점\n","\n","이번 Prompt Engineering 파트에서는 성능 기반 평가 기준 위주로 보고, 응답 속도와 비용은 프로젝트들 진행하면서 설명 예정"],"metadata":{"id":"ELC3RBpY8xA4"}},{"cell_type":"markdown","source":["### 2. 평가를 진행할 테스트 케이스 선정\n","\n","2-1. 테스트 케이스 확보\n","- 예시 시나리오: 숙소 리뷰 요약\n","\n","- 사람이 직접 제작한 N개 정도의 Golden Reference\n","- 비용 이슈로 ChatGPT 3.5를 써야한다고 했을 때 더 상위 레벨의 모델 출력 값 (ex. GPT-4)\n","- 이미 시중에 존재하는 레이블링 된 데이터 등등\n","\n","2-2. 엣지 케이스\n","- 예시 1\n","  - 입력이 매우 길거나 매우 짧은 케이스들\n","- 예시 2\n"," - 토막글이나 실제 숙소에 대한 정보가 별로 없는 리뷰 등의 저품질 리뷰들로만 이루어진 케이스들\n","- 예시 3\n","  - 입력으로 숙소 리뷰가 N개 이하 들어왔을 때는 요약 결과가 아닌 충분하지 않은 케이스\n","    - 이런 케이스는 후처리 또는 전처리로 해결하는게 나을 수도 있음"],"metadata":{"id":"lptyYyV2DBm2"}},{"cell_type":"markdown","source":["### 3. Baseline Prompt 선정\n","- 고도화 과정을 확인하기 위한 용도의 Baseline Prompt 선정\n","- 정말 단순하고 Naive 한 Prompt\n","  - 특별히 노력을 거치치 않은 Prompt\n","  - Prompt Library 중에 하나로 선정"],"metadata":{"id":"4b6CFreuDBye"}},{"cell_type":"markdown","source":["### 이후 고도화 과정\n","4. (반복) 테스트 케이스에 대해 평가 진행\n","5. (반복) Prompt 수정\n","6. Prompt 완성\n","  - 최종 Prompt 공유\n","  - Baseline Prompt와 Baseline 대비 어떤 지표에서 얼마나 개선되었는지도 공유"],"metadata":{"id":"zWc_r5DmDB10"}},{"cell_type":"markdown","source":["### 정리\n","- 최상급 품질의 Prompt 제작에는 명확하게 태스크와 평가 기준을 설정하고 진행하는게 중요"],"metadata":{"id":"nRv4H_gMHQRc"}},{"cell_type":"code","source":[],"metadata":{"id":"DiQi5zPfHSIJ"},"execution_count":null,"outputs":[]}]}
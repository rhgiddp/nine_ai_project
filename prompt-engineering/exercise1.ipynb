{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOHdL-wnSnaZ"
   },
   "source": [
    "# Prompt Engineering 파트 실습 1 - 기초 편\n",
    "\n",
    "Prompt Engineering 관련 아래 기본기들을 익힐 예정\n",
    "- 기본 용어 정리\n",
    "  - Prompt, Role, Response\n",
    "- ChatGPT 간단한 원리\n",
    "  - LLM Next Token Prediction\n",
    "- OpenAI API 호출 방법\n",
    "  - Model, Prompt Text, Temperature\n",
    "  - Max Length\n",
    "- 몇 가지 기본 규칙들\n",
    "\n",
    "*Prompt Engineering 실습 파트는 OpenAI, Anthropic Cookbook, 관련 논문들 및 컨퍼런스 등의 다양한 정보들을 취합한 내용과 강사의 경험을 토대로 진행\n",
    "\n",
    "**실습 비용은 프롬프트 엔지니어링 파트 통틀어서 1000원 미만으로 나올 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ievIxOsWSsrd"
   },
   "source": [
    "## 기본 용어 정리\n",
    "\n",
    "- Prompt: ChatGPT의 출력을 원하는 방향으로 유도하기 위한 입력 텍스트. Prompt는 보통 질문 또는 지시 형태를 나타냄\n",
    "\n",
    "| Role | Prompt |\n",
    "| --- | --- |\n",
    "| User | 왜 하늘은 하늘색인가요? |\n",
    "\n",
    "- Role: 역할. 크게는 (1) 사용자를 뜻하는 User (2) ChatGPT를 뜻하는 Assistant 그리고 (3) System이 존재\n",
    "  - System은 사용자 Prompt 이전에 입력하는 성능 개선 용도의 Prompt\n",
    "\n",
    "| Role | Prompt |\n",
    "| --- | --- |\n",
    "| Assistant | 하늘은 하늘색인 이유는 대기 중의 분자들이 태양으로부터 오는 빛을 흡수하고 산란시키기 때문입니다. 태양으로부터 오는 빛은 다양한 색상을 가지고 있는데, 대기 중의 분자들은 파란색 빛을 더 많이 흡수하고 산란시키기 때문에 하늘은 파란색으로 보이게 됩니다. 이러한 현상을 레이리 산란이라고 합니다. 따라서 하늘은 하늘색으로 보이는 것입니다. |\n",
    "\n",
    "- Response (또는 Output): 사용자의 Prompt에 대한 LLM의 출력값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8V-z7WASwd6"
   },
   "source": [
    "## 간단한 원리 설명 - ChatGPT가 말을 하는 방식에 대하여\n",
    "\n",
    "- Next Token Prediction\n",
    "- ChatGPT 같은 Large Language Model(LLM) 또는 대규모 언어 모델들은 기본적으로 정해진 수의 단어들을 알고 있습니다.\n",
    "  - 예를 들어 메타의 라마2 7B는 32000개, 구글의 젬마 7B는 256000개를 알고 있습니다. (단어를 더 많이 안다고 성능이 좋아지는 것은 아닙니다)\n",
    "- ChatGPT가 예를 들어 10만개의 단어를 알고 있다고 했을 때, 10만개 단어 중 다음 1개의 단어를 예측하는 방식입니다.\n",
    "  - 예시 1. 왜 하늘은 하늘색인가요? ___\n",
    "  - 예시 2. 왜 하늘은 하늘색인가요? 하늘은 하늘색인 이유는 대기 중의 분자들이 태양으로부터 오는 빛을 ___\n",
    "- \"단어\"란 우리가 생각하는 단어는 아닙니다. ChatGPT는 한글이나 영어 문자를 바로 보는게 아니고 이걸 AI모델이 이해 할 수 있는 숫자들로 변환해야하는데 이 숫자들의 수입니다.\n",
    "  - 공식 명칭은 tokens라고 합니다. platform.openai.com/tokenizer 웹사이트에서 tokenizer 기억 나시죠?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8eN4tv3kT7w"
   },
   "source": [
    "## Google Colab에 OpenAI API Key 등록\n",
    "- 왼쪽 열쇠 모양 버튼 클릭 (보안 비밀 / Secrets 메뉴)\n",
    "- 이름에는 OPENAI_API_KEY 입력\n",
    "- 값에는 OpenAI API Key 복붙하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2629,
     "status": "ok",
     "timestamp": 1714133970511,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "0f29uLK7ktrn"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1FWiaLWi5Ui"
   },
   "source": [
    "## OpenAI API 호출 방법\n",
    "- OpenAI API 호출 방법이지만 다른 많은 LLM API들도 OpenAI API 형식을 동일하게 따르거나 비슷한 방식의 구조를 채택했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9612,
     "status": "ok",
     "timestamp": 1714134001770,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "fJxivabOmGc3",
    "outputId": "504fc834-3d8f-4af9-afd1-c731060a3e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\kus\\miniconda3\\lib\\site-packages (1.30.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kus\\miniconda3\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\kus\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kus\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kus\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kus\\miniconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1714134036768,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "brWiz-KGSyqd"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m OPENAI_API_KEY = os.getenv(\u001b[33m'\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m OPENAI_API_KEY, \u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY가 없습니다\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOPENAI_API_KEY\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KUS\\miniconda3\\Lib\\site-packages\\openai\\_client.py:122\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    120\u001b[39m     base_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m._default_stream_cls = Stream\n\u001b[32m    135\u001b[39m \u001b[38;5;28mself\u001b[39m.completions = resources.Completions(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KUS\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:825\u001b[39m, in \u001b[36mSyncAPIClient.__init__\u001b[39m\u001b[34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[39m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    809\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    810\u001b[39m     )\n\u001b[32m    812\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    813\u001b[39m     version=version,\n\u001b[32m    814\u001b[39m     limits=limits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     _strict_response_validation=_strict_response_validation,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28mself\u001b[39m._client = http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KUS\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:723\u001b[39m, in \u001b[36m_DefaultHttpxClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mlimits\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[32m    722\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mfollow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv('.env')\n",
    "\n",
    "# API 키 확인\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "assert OPENAI_API_KEY, \"OPENAI_API_KEY가 없습니다\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4432,
     "status": "ok",
     "timestamp": 1714134265274,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "xv34Hi7wS0Qz",
    "outputId": "b8cdb86c-b77a-4d30-9ea1-01cd29d16c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 하늘색인 이유는 대기 중의 분자들이 푸른 빛을 흡수하고 다른 색상의 빛을 반사하기 때문입니다. 태양 빛은 다양한 색상의 빛으로 구성되어 있지만, 대기 중의 분자들이 푸른 빛을 흡수하고 다른 색상의 빛을 반사하게 되어 우리 눈에는 하늘이 푸르게 보이게 됩니다. 이러한 현상을 산란이라고 하며, 이로 인해 하늘은 하늘색으로 보이게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=[{'role': 'user', 'content': '왜 하늘은 하늘색인가요?'}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcGwqKOlS7R9"
   },
   "source": [
    "### 한 개씩 살펴보기\n",
    "- model: GPT 3.5, GPT-4 버전 별로 선택하실 수 있으며, 보통 더 좋은 성능의 모델일 수록 가격도 그만큼 비싼 편입니다.\n",
    "- messages: Prompt, Role, Response/Output\n",
    "- temperature: 높을 수록 동일한 Prompt에도 매번 다르게 이야기하는 경향이 강해집니다. 0.0으로 셋팅 시 같은 답변으로만 대답합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeWKS0Edh8-0"
   },
   "source": [
    "### OpenAI API 결과 조금 더 자세히 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1714135237088,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "CjY3LKnQiVMX",
    "outputId": "5e785e22-f5f1-459d-f122-eec7b7ed00be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9IEuiGjJAaLjFzXqIY8L13ScUTptI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='하늘은 하늘색인 이유는 대기 중의 분자들이 푸른 빛을 흡수하고 다른 색상의 빛을 반사하기 때문입니다. 태양 빛은 다양한 색상의 빛으로 구성되어 있지만, 대기 중의 분자들이 푸른 빛을 흡수하고 다른 색상의 빛을 반사하게 되어 우리 눈에는 하늘이 푸르게 보이게 됩니다. 이러한 현상을 산란이라고 하며, 이로 인해 하늘은 하늘색으로 보이게 됩니다.', role='assistant', function_call=None, tool_calls=None))], created=1714134260, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3b956da36b', usage=CompletionUsage(completion_tokens=184, prompt_tokens=21, total_tokens=205))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQcFKE_EiWrI"
   },
   "source": [
    "### 호출 시 지정하진 않지만 알아야 하는 내용\n",
    "- max length / context window\n",
    "  - 모델마다 입력 및 출력 최대 길이가 다르며, 보통 각 모델 소개 페이지에서 찾을 수 있습니다.\n",
    "  - OpenAI API의 경우 입력 최대 길이 확인: https://platform.openai.com/tokenizer\n",
    "    - 모델 별로 tokenizer는 다릅니다. \"왜 하늘은 하늘색인가요?\" 문장이 어떤 모델한테는 14토큰, 어떤 모델은 29토큰 일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOedQZ_rigVE"
   },
   "source": [
    "### 기본적인 Prompt 구조\n",
    "\n",
    "Prompt에는 2가지 종류가 존재\n",
    "1. 사용자가 ChatGPT한테 실제로 전달하는 Prompt = User Prompt\n",
    "2. 사용자 Prompt 이전에 오는 해당 LLM 어플리케이션에 적합한 메타 Prompt = System Prompt\n",
    "\n",
    "System Prompt란?\n",
    "- 사용자 Prompt를 전달하기 전에 관련 맥락이나 지침을 설정하는 Prompt\n",
    "  - 페르소나, 어조 등도 설정 할 수 있음\n",
    "- System Prompt 예시\n",
    "  - 출력값 지정 (ex. JSON Formatting)\n",
    "  - 페르소나 및 어조 설정\n",
    "  - 외부 정보 주입\n",
    "  - 모델이 지켜야 할 규칙들 설정\n",
    "\n",
    "왠만한 모델들은 Prompt 입력 시 기본 System Prompt가 붙어있습니다. ChatGPT도 그렇습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5904,
     "status": "ok",
     "timestamp": 1714135806551,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "RPkXvVQIijn_",
    "outputId": "48a0a38d-a86c-4a96-d87c-6976baa47842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 왜 하늘색일까요? 그 이유는 바로 태양 빛이 하늘을 통과할 때 일어나는 일 때문이에요! 태양은 다양한 색깔의 빛을 내보내는데, 그 중에서도 파란색 빛이 하늘에 가장 많이 퍼져요. 그래서 우리 눈에 보이는 하늘은 파란색으로 보이게 되는거죠! 이렇게 태양 빛이 하늘을 통과하면서 파란색 빛이 우리 눈에 가장 많이 보이게 되어서 하늘은 파란색으로 보이는거에요. 신기하죠? 하늘색은 태양 빛과 공기가 어떻게 상호작용하는지에 따라 달라질 수 있어요!\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': '당신은 물리학 선생님입니다. 초등학교 5학년한테 설명하듯이 아주 쉽고 친근하게 설명해주세요.'},\n",
    "        {'role': 'user', 'content': '왜 하늘은 하늘색인가요?'}\n",
    "    ],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8265,
     "status": "ok",
     "timestamp": 1714135874256,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "pxrRAZGgiktj",
    "outputId": "e4b24ec0-3fb9-406a-b771-a99d6154e76a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 왜 하늘색인지 궁금하시죠? 그 이유는 바로 '빛의 산란' 때문이에요! \n",
      "\n",
      "우리가 보는 빛은 여러 색으로 이루어져 있는데요, 빨간색, 주황색, 노란색, 초록색, 파란색, 남색, 보라색이 있어요. 이 중에서 하늘은 파란색으로 보이게 되는데요, 그 이유는 태양에서 나오는 빛 중에서 파란색 빛이 더 많이 산란되기 때문이에요. \n",
      "\n",
      "산란이란 빛이 공기나 물같은 물질과 부딪히면 흩어지는 현상을 말해요. 하늘에는 공기가 많이 있어서 태양에서 나오는 빛 중 파란색 빛이 더 많이 흩어지게 되어 하늘이 파란색으로 보이는 거죠! \n",
      "\n",
      "그래서 하늘은 파란색이에요. 이렇게 쉽게 설명해 드렸어요! 이해가 되셨나요? 더 궁금한 점 있으면 언제든지 물어주세요!\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': '당신은 물리학 선생님입니다. 초등학교 5학년한테 설명하듯이 아주 쉽고 친근하게 설명해주세요. 왜 하늘은 하늘색인가요?'}\n",
    "    ],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiyzTtEaio-U"
   },
   "source": [
    "### Completion 말고 Stream도 살펴보기\n",
    "- ChatGPT가 문장을 모두 완성하지 않고 각 단어 별로 완성되는데로 바로바로 보여주는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4026,
     "status": "ok",
     "timestamp": 1714136032685,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "8z_JhYKCipim",
    "outputId": "b1aad268-f0af-4038-d172-f96840b053aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘은 하늘색인 이유는 대기 중의 분자들이 햇빛을 흡수하고 산란시키기 때문입니다. 태양으로부터 오는 빛은 다양한 파장을 가지고 있는데, 대기 중의 분자들은 이 빛을 흡수하고 다시 방출하면서 산란시킵니다. 이 과정에서 파란색 빛이 다른 색보다 더 많이 산란되어 하늘은 파란색으로 보이게 됩니다. 이러한 현상을 레이리 산란이라고 합니다."
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=[{'role': 'user', 'content': '왜 하늘은 하늘색인가요?'}],\n",
    "    temperature=0.0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714136039814,
     "user": {
      "displayName": "Marko",
      "userId": "13892425088441002710"
     },
     "user_tz": -540
    },
    "id": "m7SLMA1Viqem",
    "outputId": "7aba0ed2-697b-4b5d-be4a-ed33cabba219"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.Stream at 0x7b5822f55990>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBixBk2sitNu"
   },
   "source": [
    "### 몇 가지 기본 규칙들\n",
    "\n",
    "1. Prompt는 영어로 해야 모델의 제성능을 발휘하는 편\n",
    "   - ChatGPT, Claude 같은 모델들의 학습 데이터 중 큰 비중이 영어로 추정되기 때문\n",
    "   - 학습 데이터가 공개된 라마1의 경우에도 대부분이 영어이며 한글은 극소량만 존재함\n",
    "   - 한글 출력값이 필요하더라도 영어 Prompt를 통해 한글 출력값을 유도하는게 성능이 더 좋을 수 있음\n",
    "2. AI 모델의 출력값은 입력값에 의존도가 매우 높음\n",
    "   - 잘 한 것 같은데 원하는 결과가 안 나오면 입력이 모호하거나 필요한 내용이 빠졌을 수도 있음 (그게 아닌 경우 모델한테 태스크가 너무 어려울 수는 있음)\n",
    "3. Prompt를 이렇게 저렇게 바꿨을 때 \"더 좋아보이는\" 결과보다는 특정 지표에서 유의미하게 더 좋거나 여러 번의 블라인드 테스팅을 통해 더 좋은 Prompt를 정하는 것을 추천\n",
    "   - 다음 챕터인 프롬프트 엔지니어링 라이프사이클에서 자세하게 알려드릴 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHN8jbJWiu0V"
   },
   "source": [
    "## 정리\n",
    "- Prompt, Role, Output\n",
    "  - Role은 User, Assistant(ex. ChatGPT)\n",
    "  - Prompt는 User, System Prompt\n",
    "- ChatGPT 작동 원리 = Next Token Prediction\n",
    "  - ChatGPT 같은 모든 LLM API는 단어1, 단어2, 단어3이 있을 때 단어3 뒤에 나올 가장 적합한 단어를 선택하는 식으로 출력\n",
    "- OpenAI API 호출 시 model과 messages를 지정해줘야 하며 결과 재현을 위해서는 temperature=0.0 지정을 추천\n",
    "  - 전체 결과값을 한꺼번에 받는 Completions, 실시간으로 바로바로 받을 수 있는 Stream으로 나뉨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a84WESkhiwnw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPnRlACkFlhHR4vHynyOum5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

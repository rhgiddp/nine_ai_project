{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0fyatouY/E70L+HC3ojeJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Prompt Engineering 파트 실습 3 - 1. 평가 기준 설정\n","\n","앞서 배운 Prompt Development Cycle에서 가장 중요한 평가 기준부터 알아 볼 예정"],"metadata":{"id":"H7oc9q8oIQI0"}},{"cell_type":"markdown","source":["## 평가 기준\n","- LLM이 어떠한 태스크를 어느 정도의 품질인지 정량적으로 또는 정성적으로 확인 할 수 있는 객관적인 지표\n"],"metadata":{"id":"iaz3c196i2gt"}},{"cell_type":"markdown","source":["### 왜 평가 기준이 필요할까?\n","- A가 B보다 더 좋은 것 같다 --> 아닐 수 있음\n","  - 사람마다 기준이 다를 수 있고 기준이 명확하지 않을 수 있음\n","  - 또한 실제로 많은 데이터 테스팅 시 아닐 수도 있음"],"metadata":{"id":"OANQqeeDcf2j"}},{"cell_type":"markdown","source":["### LLM 평가 지표는 존재\n","\n","1. MMLU (Massive Multitask Language Understanding) - github, paper\n","  - 여러 분야 테스트하는 객관식 시험\n","  - 참고로 MMLU (5 shot)의 경우 5개의 질문/정답 쌍이 Prompt로 주어졌다는 뜻\n","  - 깃헙: https://github.com/hendrycks/test\n","  - 논문: https://arxiv.org/pdf/2009.03300\n","2. ARC (Abstraction and Reasoning Corpus) - github, paper\n","  - 2차원 pixels grid 주고 특정 문제 해결 ex. 패턴 주고 일부 비워두고 어떤 색깔로 칠 할지 맞추는 문제\n","  - 깃헙: https://github.com/fchollet/ARC\n","  - 논문: https://arxiv.org/abs/1911.01547\n","3. HellaSwag - website, paper\n","  - 문장들 주고 이어지는 마지막 문장들로 가장 적합한 문장들 4개 중 하나 고르는 문제\n","  - 웹사이트: https://rowanzellers.com/hellaswag/\n","  - 논문: https://arxiv.org/pdf/1905.07830\n","4. TruthfulQA - github, agit\n","  - 할루시네이션 측정용 데이터셋이고 주어진 문제에 대한 Accuracy 측정 (문제 유형은 객관식 MC 외에 더 있음)\n","  - 깃헙: https://github.com/sylinrl/TruthfulQA\n","  - 논문: https://arxiv.org/abs/2109.07958\n","\n","\n","위 지표들 외에도 수학, 코딩 능력 등을 확인하는 지표들도 존재함"],"metadata":{"id":"ugj_4ZJ8jTqL"}},{"cell_type":"markdown","source":["### 태스크에 적합한 평가 지표 설정이 필요한 이유\n","- 실사용 입장에선 사실 위 지표들간의 미미한 우위는 의미 없고 특정 태스크 점수가 중요\n","  - 예를 들어, 최근에 나온 Mistral 모델이 라마2 모델보다 지표 상으로는 더 우월함\n","  - 막상 한글 Q&A 태스크에서는 반대인 상황이라 실서비스 용도로는 후자의 성능이 더 좋은 케이스가 좀 더 많은 편\n","- 즉, 위 지표가 더 높다고 특정 태스크에서 무조건 더 높은건 아님\n","  - **태스크에 적합한 평가 지표 설정이 필요한 이유!**\n","  - 범용 모델을 평가해야 하다보니 대표적인 지표들을 선정한 것"],"metadata":{"id":"6UUqMimfcb8Z"}},{"cell_type":"markdown","source":["### 평가 기준 예시\n","- 예시 1번. 인문학부터 STEM 분야까지 총 45개 주제를 포괄하는 35,030개의 전문가 수준 다지선형 선택 문제 (KMMLU)\n","  - 평가 기준 = 정확도\n","  - 매우 명확함\n","- 예시2. 요약\n","  - 요약을 잘 한다는 것을 어떻게 표현 할 수 있을까?\n","  - 정답 요약이 있을 때 정답과 최대한 비슷하게 요약하는 것이 될 수 있음\n","    - 이런 경우 Cosine Similarity나 Recall 기반 지표들 존재\n","  - 정답 요약이 없다면? 정답 요약의 품질이 좋지 않다면? 단순하게 \"더 비슷한게\" 정말 더 좋은 요약일까?"],"metadata":{"id":"1CVzlqhejxVL"}},{"cell_type":"markdown","source":["### 태스크에 적합한 평가 기법 분류\n","\n","1. 사람이 평가하는 방법\n","2. LLM 모델이 평가하는 방법\n","3. 코드로 평가하는 방법"],"metadata":{"id":"8YL55Gz0oolH"}},{"cell_type":"markdown","source":["### 1. Human Based Method\n","- 사람이 평가하는 방법\n","  - 전문가 블라인드 A/B 테스트 --> ELO 리더보드\n","  - 2가지 답변 중에 더 좋은 답변을 선택하는 방법\n","  - 명확한 평가 기준\n","\n","#### LMSys Chatbot Arena\n","- https://chat.lmsys.org/\n","- 동일한 질문에 대해 2개 모델의 모델로 답변 출력\n","- 2개의 모델의 이름은 가리고, 승/패/무 투표 이후에 모델명 공개\n","\n","![](https://drive.google.com/uc?export=view&id=1mu79XpBWd4zKzr3kXVF3iAAxMAvwFG-o)\n","\n"],"metadata":{"id":"w4HW_ZqMkV1G"}},{"cell_type":"markdown","source":["#### LMSys Chatbot Arena Leaderboard\n","- https://chat.lmsys.org/?leaderboard\n","- 50만번 이상의 모델 비교 결과를 토대로 ELO 랭킹 계산\n","\n","![](https://drive.google.com/uc?export=view&id=1URmsi4BNmfzXipXZcS6Ee1Yu8DrksmUz)"],"metadata":{"id":"SaQJRli1nyUw"}},{"cell_type":"markdown","source":["#### 2. Model Based Evaluation\n","- GPT-4 같은 Strong LLM을 통해 평가하는 방법 i.e. LLM-as-a-judge\n","  - 실제로 사람이 평가하는 것과 굉장히 유사하다는 논문 결과들 존재\n","- 평가하는 방식에는 3가지 존재 (출처: MT-Bench 논문)\n","  1. Pairwise Comparison\n","    - 질문과 답변 2개를 받아 둘 중 어떤 답변이 더 좋은 지 또는 무승부인지 답변\n","  2. Single Answer Grading\n","    - 질문과 답변이 있을 때 답변에 점수를 매기는 것\n","  3. Reference-Guided Grading\n","    - 예시 답변을 주고 점수를 매기는 것\n","\n","- MT-Bench 논문\n","  - https://arxiv.org/abs/2306.05685\n","  - GPT-4로 평가 진행\n","- G-Eval 논문\n","  - https://arxiv.org/abs/2303.16634"],"metadata":{"id":"z4VeWKFvkjPw"}},{"cell_type":"code","source":["!pip install openai --quiet"],"metadata":{"id":"NtG9NS-r1Aul","executionInfo":{"status":"ok","timestamp":1714139082819,"user_tz":-540,"elapsed":7292,"user":{"displayName":"Marko","userId":"13892425088441002710"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"gIDj3Ab_1TXT","executionInfo":{"status":"ok","timestamp":1714139103208,"user_tz":-540,"elapsed":1411,"user":{"displayName":"Marko","userId":"13892425088441002710"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from openai import OpenAI\n","\n","client = OpenAI(\n","    api_key=OPENAI_API_KEY\n",")"],"metadata":{"id":"Krk0zUHc024p","executionInfo":{"status":"ok","timestamp":1714139120092,"user_tz":-540,"elapsed":2,"user":{"displayName":"Marko","userId":"13892425088441002710"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["question = '하늘은 왜 하늘색인가요?'\n","\n","print(question)"],"metadata":{"id":"VrFXppp61GP6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714139147101,"user_tz":-540,"elapsed":5,"user":{"displayName":"Marko","userId":"13892425088441002710"}},"outputId":"0b03c3fb-7be7-4ec5-be38-36c19e7255b1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["하늘은 왜 하늘색인가요?\n"]}]},{"cell_type":"code","source":["completion = client.chat.completions.create(\n","    model='gpt-3.5-turbo-0125',\n","    messages=[{'role': 'user', 'content': question}],\n","    temperature=0.0\n",")\n","\n","answer_a = completion.choices[0].message.content\n","print(answer_a)"],"metadata":{"id":"POOAWSSp1Qgv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714139254218,"user_tz":-540,"elapsed":6645,"user":{"displayName":"Marko","userId":"13892425088441002710"}},"outputId":"42a67ac4-8625-4bd4-dd90-31db3c8ad8b5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["하늘은 파란색으로 보이는데, 이는 태양 빛이 대기 중의 공기 분자들과 상호 작용하여 파란색 빛을 흡수하고 반사하기 때문입니다. 이러한 현상을 산란이라고 하는데, 파란색 빛이 다른 색상의 빛보다 더 많이 산란되기 때문에 하늘이 파란색으로 보이게 됩니다. 따라서 우리가 보는 하늘의 색은 이러한 빛의 산란 현상으로 인해 파란색으로 보이게 되는 것입니다.\n"]}]},{"cell_type":"code","source":["completion = client.chat.completions.create(\n","    model='gpt-4-1106-preview',\n","    messages=[{'role': 'user', 'content': question}],\n","    temperature=0.0\n",")\n","\n","answer_b = completion.choices[0].message.content\n","print(answer_b)"],"metadata":{"id":"f_Cd5G6n1iIi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714139309964,"user_tz":-540,"elapsed":31366,"user":{"displayName":"Marko","userId":"13892425088441002710"}},"outputId":"dab9b644-c5d2-4ccf-b749-c661a970f5ac"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["하늘의 색깔이 대체로 하늘색, 즉 푸른색으로 보이는 이유는 대기 중의 분자들에 의한 빛의 산란 현상 때문입니다. 이 현상은 '레일리 산란(Rayleigh scattering)'이라고 불리며, 대기를 통과하는 태양광이 공기 분자, 먼지, 수증기 등에 의해 산란되는 과정에서 발생합니다.\n","\n","레일리 산란은 파장이 짧은 빛(예: 파란색, 보라색)이 파장이 긴 빛(예: 빨간색, 주황색)보다 더 쉽게 산란되는 성질을 가지고 있습니다. 태양광은 다양한 색의 빛으로 구성되어 있는데, 이 중 파란색 빛의 파장이 가장 짧기 때문에 대기 중의 분자들에 의해 가장 강하게 산란됩니다. 그 결과, 하늘은 우리 눈에 파란색으로 보이게 됩니다.\n","\n","그러나 하늘의 색깔은 하루 중 시간과 대기의 상태에 따라 변할 수 있습니다. 예를 들어, 해가 지평선에 가까울 때(일출이나 일몰 시)는 대기를 통과하는 빛의 경로가 길어지기 때문에 파란색 빛은 대부분 산란되어 사라지고, 빨간색이나 주황색 빛이 더 강하게 산란되어 하늘이 붉게 보이게 됩니다. 또한 구름이 많거나 대기 오염이 심할 때는 다른 색깔의 빛이 산란되어 하늘의 색깔이 달라질 수 있습니다.\n"]}]},{"cell_type":"code","source":["# 평가 프롬프트 출처: MT-Bench 논문 https://arxiv.org/pdf/2306.05685.pdf (A. Prompt Templates Figure 5)\n","\n","prompt = f\"\"\"[System]\n","Please act as an impartial judge and evaluate the quality of the responses provided by two\n","AI assistants to the user question displayed below. You should choose the assistant that\n","follows the user’s instructions and answers the user’s question better. Your evaluation\n","should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n","and level of detail of their responses. Begin your evaluation by comparing the two\n","responses and provide a short explanation. Avoid any position biases and ensure that the\n","order in which the responses were presented does not influence your decision. Do not allow\n","the length of the responses to influence your evaluation. Do not favor certain names of\n","the assistants. Be as objective as possible. After providing your explanation, output your\n","final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n","if assistant B is better, and \"[[C]]\" for a tie.\n","\n","[User Question]\n","{question}\n","\n","[The Start of Assistant A’s Answer]\n","{answer_a}\n","[The End of Assistant A’s Answer]\n","\n","[The Start of Assistant B’s Answer]\n","{answer_b}\n","[The End of Assistant B’s Answer]\"\"\"\n","\n","print(prompt)"],"metadata":{"id":"Q1r5eFuuzueC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714139357268,"user_tz":-540,"elapsed":415,"user":{"displayName":"Marko","userId":"13892425088441002710"}},"outputId":"3c8b82d8-d541-45e8-a978-9ef5b9bf4c08"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[System]\n","Please act as an impartial judge and evaluate the quality of the responses provided by two\n","AI assistants to the user question displayed below. You should choose the assistant that\n","follows the user’s instructions and answers the user’s question better. Your evaluation\n","should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n","and level of detail of their responses. Begin your evaluation by comparing the two\n","responses and provide a short explanation. Avoid any position biases and ensure that the\n","order in which the responses were presented does not influence your decision. Do not allow\n","the length of the responses to influence your evaluation. Do not favor certain names of\n","the assistants. Be as objective as possible. After providing your explanation, output your\n","final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n","if assistant B is better, and \"[[C]]\" for a tie.\n","\n","[User Question]\n","하늘은 왜 하늘색인가요?\n","\n","[The Start of Assistant A’s Answer]\n","하늘은 파란색으로 보이는데, 이는 태양 빛이 대기 중의 공기 분자들과 상호 작용하여 파란색 빛을 흡수하고 반사하기 때문입니다. 이러한 현상을 산란이라고 하는데, 파란색 빛이 다른 색상의 빛보다 더 많이 산란되기 때문에 하늘이 파란색으로 보이게 됩니다. 따라서 우리가 보는 하늘의 색은 이러한 빛의 산란 현상으로 인해 파란색으로 보이게 되는 것입니다.\n","[The End of Assistant A’s Answer]\n","\n","[The Start of Assistant B’s Answer]\n","하늘의 색깔이 대체로 하늘색, 즉 푸른색으로 보이는 이유는 대기 중의 분자들에 의한 빛의 산란 현상 때문입니다. 이 현상은 '레일리 산란(Rayleigh scattering)'이라고 불리며, 대기를 통과하는 태양광이 공기 분자, 먼지, 수증기 등에 의해 산란되는 과정에서 발생합니다.\n","\n","레일리 산란은 파장이 짧은 빛(예: 파란색, 보라색)이 파장이 긴 빛(예: 빨간색, 주황색)보다 더 쉽게 산란되는 성질을 가지고 있습니다. 태양광은 다양한 색의 빛으로 구성되어 있는데, 이 중 파란색 빛의 파장이 가장 짧기 때문에 대기 중의 분자들에 의해 가장 강하게 산란됩니다. 그 결과, 하늘은 우리 눈에 파란색으로 보이게 됩니다.\n","\n","그러나 하늘의 색깔은 하루 중 시간과 대기의 상태에 따라 변할 수 있습니다. 예를 들어, 해가 지평선에 가까울 때(일출이나 일몰 시)는 대기를 통과하는 빛의 경로가 길어지기 때문에 파란색 빛은 대부분 산란되어 사라지고, 빨간색이나 주황색 빛이 더 강하게 산란되어 하늘이 붉게 보이게 됩니다. 또한 구름이 많거나 대기 오염이 심할 때는 다른 색깔의 빛이 산란되어 하늘의 색깔이 달라질 수 있습니다.\n","[The End of Assistant B’s Answer]\n"]}]},{"cell_type":"code","source":["completion = client.chat.completions.create(\n","    model='gpt-4-turbo-2024-04-09',\n","    messages=[{'role': 'user', 'content': prompt}],\n","    temperature=0.0\n",")\n","\n","print(completion.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UT0akplSqvPJ","executionInfo":{"status":"ok","timestamp":1714139434338,"user_tz":-540,"elapsed":15005,"user":{"displayName":"Marko","userId":"13892425088441002710"}},"outputId":"f5b31826-cddd-4264-8a93-3d9ebfb887bd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing the responses from Assistant A and Assistant B, both assistants provide accurate and relevant explanations for why the sky appears blue, citing the phenomenon of light scattering in the atmosphere. However, there are notable differences in the depth and detail of their explanations.\n","\n","Assistant A's response is straightforward and correctly explains the basic concept of scattering, specifically mentioning that blue light is scattered more than other colors, which is why the sky appears blue. This explanation is accurate but somewhat limited in scope.\n","\n","Assistant B, on the other hand, not only explains the scattering phenomenon but also introduces the specific term \"Rayleigh scattering,\" which adds a layer of scientific precision to the response. Assistant B further elaborates on how different conditions, such as the time of day and atmospheric conditions, can affect the color of the sky, providing examples of how the sky can appear red or orange during sunrise or sunset and how it can vary with weather conditions or pollution. This response is not only scientifically accurate but also more comprehensive and informative, offering the user a deeper understanding of the topic.\n","\n","In terms of helpfulness, relevance, accuracy, depth, creativity, and level of detail, Assistant B's response excels by providing a more thorough explanation and by contextualizing the phenomenon in everyday observations, which enhances the educational value of the answer.\n","\n","Based on these considerations, the verdict is:\n","[[B]]\n"]}]},{"cell_type":"markdown","source":["#### 3. Code Based Evaluation\n","- 우리한테 익숙한 코드/로직을 통한 평가 방법\n","  - Accuracy, Precision, Recall...\n","  - ROUGE: A Package for Automatic Evaluation of Summaries\n","  - BLEU: a Method for Automatic Evaluation of Machine Translation\n","  - Exact Match, String Match"],"metadata":{"id":"Ix3cr0Dtki2d"}},{"cell_type":"markdown","source":["### 장단점 비교\n","\n","1. Human Based Evalution\n","  - 통제된 환경을 가정 했을 때 사람이 직접 평가한 방법이라 안정적이고 신뢰 할 수 있음\n","  - 불특정 다수의 경우 약간의 노이즈 발생 가능\n","  - 전문 도메인의 경우 해당 도메인 전문가가 아닌 일반인이 평가 할 경우 정확도 및 평가 속도가 낮아질 수 있음\n","2. Model Based Evaluation\n","  - 사람 평가와 어느 정도 유사한 수준의 평가를 내릴 수 있음\n","  - 평가를 위해 API 호출이 필요한데 평가 데이터가 굉장히 많을 경우 천만원 이상은 금방 넘어 갈 수 있음\n","3. Code Based Evaluation\n","  - 위 방법들과 인력 고용 비용, 모델 호출 비용 등이 없는 무료 평가 방법\n","  - 태스크에 따라 위 방법들보다 더 정확 할 수도 있고 그러지 않을 수도 있음\n","  - 정확도 같은 지표를 벗어나 사람한테 적합한 답변을 선택하는데 있어서는 신뢰도가 상대적으로 떨어지는 편"],"metadata":{"id":"hYNN-8JIqa1E"}},{"cell_type":"markdown","source":["### 결론\n","- 태스크에 적합한 사람이 평가하는 방법이 가장 좋음\n","- 그러나 현실적으로 모델이 평가하는 방법도 충분히 사용 가능\n","- 비용 및 속도 이슈가 없는 코드 기반 평가는 자주 활용하는 것을 권장\n","- 정량적인 평가와 정성적인 평가 모두 하는게 가장 이상적인 케이스\n","- **실서비스로 봤을 때 최종 지표는 결국 사용자 피드백**\n","  - 고도화 과정을 명확하게 객관적으로 observe 하기 위해서 명확한 평가 기준을 잡는 것"],"metadata":{"id":"LctV0q1loy9e"}},{"cell_type":"markdown","source":["### 정리\n","- LLM 평가하는 공식적인 방법론은 존재함\n","  - 그러나 이 방법론들은 범용 모델의 평가 방법론이라 태스크에 적합한 평가 기준 설정이 필요\n","- 평가 기준에는 총 3가지 방법이 존재함\n","  1. 사람이 평가하는 방법\n","  2. 모델이 평가하는 방법 (ex. MT-Bench, G-Eval)\n","  3. 코드로 평가하는 방법 (ex. Accuracy, ROUGE)"],"metadata":{"id":"RqHTFaJam4pb"}}]}
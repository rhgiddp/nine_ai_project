# GPU ì‚¬ìš© ê°€ì´ë“œ

ëª¨ë“  í”„ë¡œì íŠ¸ì—ì„œ GPU(PyTorch)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì„¤ì • ì™„ë£Œ ì‚¬í•­

### 1. PyTorch í™˜ê²½ êµ¬ì„±
- **í™˜ê²½ ì´ë¦„**: `pytorch_gpu_312`
- **Python ë²„ì „**: 3.12.11
- **PyTorch ë²„ì „**: 2.5.1 (CUDA 12.1 ì§€ì›)
- **GPU**: NVIDIA GeForce RTX 3060 Laptop GPU (6GB)

### 2. ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€
```bash
conda activate pytorch_gpu_312
```

ì£¼ìš” íŒ¨í‚¤ì§€:
- `torch==2.5.1+cu121`
- `torchvision==0.20.1+cu121`
- `torchaudio==2.5.1+cu121`
- `openai`, `anthropic`, `google-generativeai`
- `gradio`, `python-dotenv`

### 3. ëª¨ë“  í”„ë¡œì íŠ¸ requirements.txt ì—…ë°ì´íŠ¸
ë‹¤ìŒ í”„ë¡œì íŠ¸ë“¤ì˜ `requirements.txt`ì— PyTorchê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤:
- `assistant-question-answering/`
- `baemin-recommendation/`
- `kakaotalk-summarization/`
- `prompt-engineering/`
- `retrieval-augmented-generation/`
- `yanolja-summarization/`

## ğŸ“ ì‚¬ìš© ë°©ë²•

### 1. Conda í™˜ê²½ í™œì„±í™”
```bash
conda activate pytorch_gpu_312
```

### 2. Jupyter ë…¸íŠ¸ë¶ì—ì„œ GPU ì‚¬ìš©

#### ë°©ë²• 1: gpu_utils.py ì‚¬ìš© (ê¶Œì¥)
```python
import sys
sys.path.append('..')  # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€

from gpu_utils import get_device, print_gpu_info

# GPU ì •ë³´ í™•ì¸
print_gpu_info()

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = get_device(prefer_gpu=True)

# ëª¨ë¸/ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
# model = model.to(device)
# data = data.to(device)
```

#### ë°©ë²• 2: ì§ì ‘ ì„¤ì •
```python
import torch

# GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
print(f'CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
print(f'GPU ì´ë¦„: {torch.cuda.get_device_name(0)}')

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}')

# í…ì„œë¥¼ GPUë¡œ ì´ë™
x = torch.randn(100, 100).to(device)
```

### 3. Python ìŠ¤í¬ë¦½íŠ¸ì—ì„œ GPU ì‚¬ìš©
```python
from gpu_utils import get_device, set_seed, clear_gpu_memory

# ì‹œë“œ ì„¤ì • (ì¬í˜„ì„±)
set_seed(42)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = get_device(prefer_gpu=True)

# ëª¨ë¸ í•™ìŠµ/ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
clear_gpu_memory()
```

## ğŸ”§ GPU ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (gpu_utils.py)

### ì£¼ìš” í•¨ìˆ˜:
- `get_device(prefer_gpu=True, device_id=0)`: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ë°˜í™˜
- `get_gpu_info()`: GPU ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
- `print_gpu_info()`: GPU ì •ë³´ ì¶œë ¥
- `set_seed(seed=42)`: ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
- `clear_gpu_memory()`: GPU ë©”ëª¨ë¦¬ ì •ë¦¬

### í…ŒìŠ¤íŠ¸:
```bash
conda activate pytorch_gpu_312
python gpu_utils.py
```

## ğŸ’¡ ì£¼ì˜ì‚¬í•­

### 1. OpenAI APIëŠ” GPU ë¶ˆí•„ìš”
í˜„ì¬ í”„ë¡œì íŠ¸ë“¤ì€ ì£¼ë¡œ **OpenAI API**ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë¡œì»¬ GPUê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- OpenAI APIëŠ” í´ë¼ìš°ë“œì—ì„œ ì‹¤í–‰ë¨
- ë¡œì»¬ GPUëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

### 2. GPUê°€ í•„ìš”í•œ ê²½ìš°
ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— GPUê°€ ìœ ìš©í•©ë‹ˆë‹¤:
- **ë¡œì»¬ LLM ì‹¤í–‰** (Llama, Mistral ë“±)
- **ì„ë² ë”© ëª¨ë¸ ë¡œì»¬ ì‹¤í–‰**
- **íŒŒì¸íŠœë‹/í•™ìŠµ**
- **ëŒ€ëŸ‰ì˜ í…ì„œ ì—°ì‚°**

### 3. GPU ë©”ëª¨ë¦¬ ì œí•œ
- RTX 3060 Laptop GPU: **6GB VRAM**
- ì‘ì€ ëª¨ë¸(7B ì´í•˜)ë§Œ ì‹¤í–‰ ê°€ëŠ¥
- í° ëª¨ë¸ì€ ì–‘ìí™”(quantization) í•„ìš”

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

### ë¡œì»¬ LLM ì‹¤í–‰í•˜ê¸°
```bash
conda activate pytorch_gpu_312
pip install transformers accelerate bitsandbytes
```

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

device = torch.device('cuda')

# ì‘ì€ ëª¨ë¸ ë¡œë“œ (ì˜ˆ: GPT-2)
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

# í…ìŠ¤íŠ¸ ìƒì„±
inputs = tokenizer("Hello, my name is", return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))
```

### ì„ë² ë”© ëª¨ë¸ ë¡œì»¬ ì‹¤í–‰
```bash
pip install sentence-transformers
```

```python
from sentence_transformers import SentenceTransformer
import torch

device = torch.device('cuda')
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

sentences = ["This is a test sentence", "Another example"]
embeddings = model.encode(sentences)
print(embeddings.shape)
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### GPU vs CPU (1000x1000 í–‰ë ¬ ê³±ì…ˆ)
- **GPU (RTX 3060)**: ~36ms
- **CPU**: ~100-200ms (ì•½ 3-5ë°° ëŠë¦¼)

## ğŸ”— ì°¸ê³  ìë£Œ
- [PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/index.html)
- [CUDA ì„¤ì¹˜ ê°€ì´ë“œ](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)

## â“ ë¬¸ì œ í•´ê²°

### GPUê°€ ì¸ì‹ë˜ì§€ ì•ŠëŠ” ê²½ìš°
```bash
# CUDA í™•ì¸
nvidia-smi

# PyTorch CUDA í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
```

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
# ëª¨ë¸ ì–‘ìí™” ì‚¬ìš©
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()
```

### Python ë²„ì „ ë¬¸ì œ
- PyTorchëŠ” Python 3.8-3.12 ì§€ì›
- Python 3.13ì€ ì•„ì§ ë¯¸ì§€ì› (2025ë…„ ì´ˆ í˜„ì¬)
- `pytorch_gpu_312` í™˜ê²½ì€ Python 3.12.11 ì‚¬ìš©

---

**ì„¤ì • ì™„ë£Œì¼**: 2025-10-06
**GPU**: NVIDIA GeForce RTX 3060 Laptop GPU (6GB)
**CUDA**: 12.7 (Driver) / 12.1 (PyTorch)

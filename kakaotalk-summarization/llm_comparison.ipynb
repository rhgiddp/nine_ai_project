{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM API 비교\n",
    "- 비용으로 1차적으로 추린 후에 2차적으로 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API\n",
    "- 다른 LLM 라인업 대비 항상 좀 더 저렴한 가격에 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 가능한 Gemini 모델들:\n",
      "- models/gemini-2.5-pro-preview-03-25\n",
      "- models/gemini-2.5-flash-preview-05-20\n",
      "- models/gemini-2.5-flash\n",
      "- models/gemini-2.5-flash-lite-preview-06-17\n",
      "- models/gemini-2.5-pro-preview-05-06\n",
      "- models/gemini-2.5-pro-preview-06-05\n",
      "- models/gemini-2.5-pro\n",
      "- models/gemini-2.0-flash-exp\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-exp-image-generation\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-preview-image-generation\n",
      "- models/gemini-2.0-flash-lite-preview-02-05\n",
      "- models/gemini-2.0-flash-lite-preview\n",
      "- models/gemini-2.0-pro-exp\n",
      "- models/gemini-2.0-pro-exp-02-05\n",
      "- models/gemini-exp-1206\n",
      "- models/gemini-2.0-flash-thinking-exp-01-21\n",
      "- models/gemini-2.0-flash-thinking-exp\n",
      "- models/gemini-2.0-flash-thinking-exp-1219\n",
      "- models/gemini-2.5-flash-preview-tts\n",
      "- models/gemini-2.5-pro-preview-tts\n",
      "- models/learnlm-2.0-flash-experimental\n",
      "- models/gemma-3-1b-it\n",
      "- models/gemma-3-4b-it\n",
      "- models/gemma-3-12b-it\n",
      "- models/gemma-3-27b-it\n",
      "- models/gemma-3n-e4b-it\n",
      "- models/gemma-3n-e2b-it\n",
      "- models/gemini-flash-latest\n",
      "- models/gemini-flash-lite-latest\n",
      "- models/gemini-pro-latest\n",
      "- models/gemini-2.5-flash-lite\n",
      "- models/gemini-2.5-flash-image-preview\n",
      "- models/gemini-2.5-flash-image\n",
      "- models/gemini-2.5-flash-preview-09-2025\n",
      "- models/gemini-2.5-flash-lite-preview-09-2025\n",
      "- models/gemini-robotics-er-1.5-preview\n",
      "- models/gemini-2.5-computer-use-preview-10-2025\n",
      "\n",
      "사용할 모델: gemini-2.0-flash-001\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from utils import get_googleai_key\n",
    "\n",
    "# API 키 설정\n",
    "GOOGLE_API_KEY = get_googleai_key()\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 사용 가능한 Gemini 모델 확인\n",
    "print(\"사용 가능한 Gemini 모델들:\")\n",
    "for model in genai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(f\"- {model.name}\")\n",
    "\n",
    "# 올바른 모델로 재초기화\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-001')\n",
    "print(f\"\\n사용할 모델: gemini-2.0-flash-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답: Hi there! How can I help you today?\n",
      "\n",
      "사용량 정보: prompt_token_count: 2\n",
      "candidates_token_count: 11\n",
      "total_token_count: 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gemini API 테스트\n",
    "try:\n",
    "    response = model.generate_content(\"Hi!\")\n",
    "    print(\"응답:\", response.text)\n",
    "    print(\"사용량 정보:\", response.usage_metadata)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n",
    "    print(\"사용 가능한 모델을 다시 확인해보세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Hi!\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude API\n",
    "- OpenAI에 필적하는 성능\n",
    "  - 2024년 6월 기준 가장 좋은 성능의 모델 = Claude 3.5 Sonnet\n",
    "- 현재 기준 가장 비싼 모델인 Claude 3 Opus 보유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01manthropic\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_anthropicai_key\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m client = \u001b[43manthropic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAnthropic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_anthropicai_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m message = client.messages.create(\n\u001b[32m     12\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mclaude-3-haiku-20240307\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     max_tokens=\u001b[32m1000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     ]\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(message.content[\u001b[32m0\u001b[39m].text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\FAST_CAMF\\Nine_Project_Github\\kakaotalk-summarization\\.conda-kakao\\Lib\\site-packages\\anthropic\\_client.py:121\u001b[39m, in \u001b[36mAnthropic.__init__\u001b[39m\u001b[34m(self, api_key, auth_token, base_url, timeout, max_retries, default_headers, default_query, http_client, transport, proxies, connection_pool_limits, _strict_response_validation)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    119\u001b[39m     base_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api.anthropic.com\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_pool_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28mself\u001b[39m._default_stream_cls = Stream\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m.completions = resources.Completions(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\FAST_CAMF\\Nine_Project_Github\\kakaotalk-summarization\\.conda-kakao\\Lib\\site-packages\\anthropic\\_base_client.py:825\u001b[39m, in \u001b[36mSyncAPIClient.__init__\u001b[39m\u001b[34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[39m\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    809\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    810\u001b[39m     )\n\u001b[32m    812\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    813\u001b[39m     version=version,\n\u001b[32m    814\u001b[39m     limits=limits,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     _strict_response_validation=_strict_response_validation,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28mself\u001b[39m._client = http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\FAST_CAMF\\Nine_Project_Github\\kakaotalk-summarization\\.conda-kakao\\Lib\\site-packages\\anthropic\\_base_client.py:723\u001b[39m, in \u001b[36m_DefaultHttpxClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mlimits\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[32m    722\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mfollow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "# 환경변수에 ANTHROPIC_API_KEY 추가 필요\n",
    "# ~/.zshrc에 export ANTHROPIC_API_KEY='<API KEY>'\n",
    "import os\n",
    "import anthropic\n",
    "from utils import get_anthropicai_key\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=get_anthropicai_key()\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    system=\"Respond only in Yoda-speak.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동일한 Prompt에 대해 Input / Output 계산\n",
    "- 비용 계산을 위해서는 Tokens 수를 알아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import get_eval_data\n",
    "\n",
    "conversations = get_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P01: 맞다요즘 싸이월드 새로 생겼다매\n",
      "P02: 나 싸이월드 복구하고 싶다\n",
      "P01: 웅 키키귀여우뉴사진\n",
      "P02: 진짜 추억돋네\n",
      "P01: 짱많아 옛날 사진들\n",
      "P01: 맨날 퍼가요~ 이거 했자나\n",
      "P02: 맞아 싸이월드 미니미\n",
      "P01: 추억 돋아서 너무조아\n",
      "P02: 꾸미는거\n",
      "P01: 마자\n",
      "P02: 재밌었는데\n",
      "P01: 귀여워 도토리충전\n",
      "P02: 네이트온도 하고\n",
      "P02: 도토리도 환불해준대자나\n",
      "P01: 마자 키키개웃겨\n",
      "P01: 요즘 사회적으로 멀 자꾸하나바\n",
      "P02: 대박이야 진짜\n",
      "P02: 그니깐 키키\n",
      "P01: 아니 나오늘 엄마가 뭐 신청해달라그래서\n",
      "P02: 웅\n",
      "P01: 지원금 ? 신청함\n",
      "P01: 진짜 요즘 지원금 엄청 많이 받았어\n",
      "P02: 와 대박\n",
      "P01: 뭔지는 잘 모르는데 이것저것 지원해주는거 많아서 좋은듯\n",
      "P02: 다행이다\n",
      "P01: 국가제도가 진짜 괜차나\n",
      "P01: 학교에서도\n",
      "P02: 두분다\n",
      "P01: 맨날 장학금 해준다고\n",
      "P02: 일을 못하는\n",
      "P01: 이것저것 지원해주자나\n",
      "P02: 상황이셔서 더\n",
      "P01: 마자ㅠㅠ\n",
      "P02: 도움주시고\n",
      "P01: 나 그래서 진짜 그런거로\n",
      "P02: 다행이다그래도\n",
      "P01: 돈 마니받앗어\n",
      "P02: 와대박\n"
     ]
    }
   ],
   "source": [
    "print(conversations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini Pro & Flash\n",
    "- 506 / 116 / 622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 사람은 싸이월드 부활에 대한 추억을 공유하며 옛날 사진과 미니미, 도토리 등을 떠올립니다. 또한, 최근 사회적으로 지원금이 많아졌다는 이야기를 나누며, 학교에서도 장학금 등 다양한 지원을 받을 수 있어 다행이라고 생각합니다. 특히, 어려운 상황에 있는 사람들에게 도움이 되는 제도가 많아져서 긍정적인 반응을 보입니다. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prompt_token_count: 506\n",
       "candidates_token_count: 116\n",
       "total_token_count: 622"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"아래 사용자 대화에 대해 3문장 내로 요약해주세요:\n",
    "\n",
    "{conversations[0]}\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-001')\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 호출해보기 전에 Prompt가 몇 토큰인 지 미리 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_tokens: 505"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제로는 여기에 token 1씩 더 추가됨\n",
    "model.count_tokens(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 Prompt에 대해 Gemini Pro와 Gemini Flash 둘 다 506 입력 토큰 -> 동일한 Tokenizer를 사용하는 것을 알 수 있음\n",
    "- 더 많은 단어를 익힌 (보통 더 큰) 모델의 경우 다른 Tokenizer를 써서 입력 토큰 수가 달라질 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 사람은 과거 싸이월드를 추억하며 미니미, 도토리 등을 회상하며 즐거워합니다. 대화 중에 최근 정부와 학교에서 지원금과 장학금을 많이 받았다는 내용이 나오고, 이에 대해 감사하며 국가 제도에 대한 긍정적인 평가를 내립니다. 전반적으로 과거 추억과 현재의 긍정적인 상황을 공유하며 가볍고 즐거운 분위기입니다. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prompt_token_count: 506\n",
       "candidates_token_count: 116\n",
       "total_token_count: 622"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"아래 사용자 대화에 대해 3문장 내로 요약해주세요:\n",
    "\n",
    "{conversations[0]}\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-pro-001')\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anthropic Claude\n",
    "- 637 / 143 / 780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 두 사람은 과거 싸이월드 사용 경험을 회상하며 추억을 공유하고 있다.\n",
      "2. 최근 정부와 학교에서 제공하는 다양한 지원금 혜택에 대해 이야기하고 있다.\n",
      "3. 경제적 어려움을 겪고 있는 상황에서 이러한 지원이 도움이 되고 있다고 말하고 있다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Usage(input_tokens=637, output_tokens=143)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.environ['ANTHROPIC_API_KEY']\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)\n",
    "message.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약:\n",
      "\n",
      "1) 싸이월드와 같은 과거 인기 커뮤니티 사이트에 대한 추억을 회상하며 그 시절의 재미있던 경험을 공유했습니다.\n",
      "\n",
      "2) 최근 정부와 학교에서 제공하는 다양한 지원금 제도에 대해 이야기하며 본인도 여러 지원금을 받았다고 말했습니다. \n",
      "\n",
      "3) 어려운 상황에 처한 사람들을 위해 국가 차원의 지원 제도가 잘 마련되어 있어 다행이라고 평가했습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Usage(input_tokens=637, output_tokens=200)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)\n",
    "message.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 싸이월드가 새로 생겨서 옛날 추억을 되새기며 즐거워하는 두 사람의 대화이다.\n",
      "2. 요즘 사회적으로 여러 지원금과 장학금 등 국가 제도의 혜택이 많다는 것에 만족해하고 있다.\n",
      "3. 두 사람 모두 일을 못하는 상황이라 이러한 지원이 더욱 도움이 된다며 다행스러워하고 있다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Usage(input_tokens=637, output_tokens=164)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)\n",
    "message.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI\n",
    "- 622 / 177 (GPT 3.5, GPT-4)\n",
    "- 441 / 76 (GPT-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 사용자는 싸이월드를 통해 추억을 공유하고, 최근 받은 지원금에 대해 이야기하며 국가제도에 대한 긍정적인 평가를 나누었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=69, prompt_tokens=622, total_tokens=691)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo-0125',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "completion.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자들은 최근 재개된 싸이월드에 대해 이야기하며, 옛날 사진과 미니미 꾸미기 등 추억을 회상합니다. 또한, 도토리 충전과 같은 싸이월드의 특징적인 요소들에 대해 언급하며 재미있었던 경험을 공유합니다. 대화에서는 또한 최근 받은 지원금과 장학금에 대한 이야기도 나누며, 이러한 지원이 도움이 되고 있다고 언급합니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=177, prompt_tokens=622, total_tokens=799)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4-turbo-2024-04-09',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "completion.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자들은 싸이월드의 복구와 관련된 추억을 이야기하며, 옛날 사진과 미니미 꾸미기, 도토리 충전 등의 재미를 회상하고 있습니다. 또한, 최근 정부의 다양한 지원금 제도에 대해 이야기하며, 이를 통해 많은 도움을 받고 있다는 점을 긍정적으로 평가하고 있습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=76, prompt_tokens=441, total_tokens=517)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model='gpt-4o-2024-05-13',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "completion.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Tokens 수 정리 (Prompt 고정)\n",
    "- Gemini 506\n",
    "- Claude 637\n",
    "- GPT 622\n",
    "- GPT-4o 441\n",
    "  - Output은 어느 정도 조정이 가능해서 Input만 비교\n",
    "\n",
    "한글 효율화 측면에서는 GPT-4o > Gemini > GPT-3.5/4 >= Claude 순\n",
    "- 다른 Prompt에서는 조금씩 다를 수 있으나, 경향이 크게 달라지지 않을 것으로 예상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비용 계산\n",
    "- Input Tokens는 바로 위에 수치 사용하고, Output Tokens는 가장 많은 Claude를 200 Tokens로 두고 비례해서 계산\n",
    "  - ex. Gemini 159 Tokens\n",
    "- 호출 횟수 별 가격표\n",
    "  - https://docs.google.com/spreadsheets/d/1L4jfa8F7ifSbp779pInSRkT_JFnT57xsmJvOhvePOk4/edit?usp=sharing\n",
    "\n",
    "모델 별 가격 (1M Token, 2024.06)\n",
    "- Claude 3 Opus $15.00 / $75.00\n",
    "- Claude 3 Sonnet $3.00 / $15.00\n",
    "- Claude 3 Haiku $0.25 / $1.25\n",
    "- Gemini Pro  $3.5 / $10.5 (일정 요청량까지 무료)\n",
    "- Gemini Flash $0.35 / $1.05 (일정 요청량까지 무료)\n",
    "- GPT-4o $5 / $15\n",
    "- GPT-4 $10 / $30\n",
    "- GPT-3.5 $0.5 / $1.5\n",
    "\n",
    "1회 호출 비용\n",
    "- Claude 3 Opus  $0.024 / 33.89원\n",
    "- Claude 3 Sonnet $0.004 / 6.78원\n",
    "- Claude 3 Haiku $0.0004 / 0.56원\n",
    "- Gemini Pro $0.003\t/ 4.75원\n",
    "- Gemini Flash $0.0003 / 0.47원\n",
    "- GPT-4o $0.004\t/ 5.91원\n",
    "- GPT-4 $0.012 / 16.67원\n",
    "- GPT-3.5 $0.0006 / 0.83원\n",
    "\n",
    "10000회 호출 비용\n",
    "- Claude 3 Opus  $245.55 / 338,859원\n",
    "- Claude 3 Sonnet $49.11 / 67,772원\n",
    "- Claude 3 Haiku $4.09 / 5,648원\n",
    "- Gemini Pro $34.39\t/ 47,460원\n",
    "- Gemini Flash $3.44 / 4,746원\n",
    "- GPT-4o $42.82\t/ 59,091원\n",
    "- GPT-4 $120.79 / 166,686원\n",
    "- GPT-3.5 $6.04 / 8,334원\n",
    "\n",
    "100만회 호출 비용\n",
    "- Claude 3 Opus  $24,555 / 33,885,900원\n",
    "- Claude 3 Sonnet $4,911 / 6,777,180원\n",
    "- Claude 3 Haiku $409.25 / 564,765원\n",
    "- Gemini Pro $3,439.13 / 4,746,002원\n",
    "- Gemini Flash $343.91 / 474,600원\n",
    "- GPT-4o $4,281.92 / 5,909,054원\n",
    "- GPT-4 $12,078.71 / 16,668,624원\n",
    "- GPT-3.5 $603.94 / 833,431원\n",
    "\n",
    "5000만회 호출 비용\n",
    "- Claude 3 Opus  $1,227,750 / 1,694,295,000원\n",
    "- Claude 3 Sonnet $245,550 / 338,859,000원\n",
    "- Claude 3 Haiku $20,462.50 / 28,238,250원\n",
    "- Gemini Pro $171,956.59 / 237,300,099원\n",
    "- Gemini Flash $17,195.66 / 23,730,010원\n",
    "- GPT-4o $214,096.15 / 295,452,692원\n",
    "- GPT-4 $603,935.64 / 833,431,177원\n",
    "- GPT-3.5 $30,196.78 / 41,671,559원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
